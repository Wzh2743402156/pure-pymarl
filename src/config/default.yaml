# --- 默认配置 ---

# --- pymarl 选项 ---
runner: "episode"              # 每个回合运行 1 个环境
mac: "basic_mac"               # 基本控制器
env: "sc2"                     # 环境名称
env_args: {}                   # 环境参数
batch_size_run: 1              # 并行运行环境的数量
test_nepisode: 20              # 测试时运行的回合数
test_interval: 2000            # 每经过 {} 个时间步后进行一次测试
test_greedy: True              # 使用贪心策略进行评估（如果为 False，则 epsilon 下限设为 0）
log_interval: 2000             # 每经过 {} 个时间步记录一次统计摘要
runner_log_interval: 2000      # 每经过 {} 个时间步记录一次 Runner 的统计（非测试统计）
learner_log_interval: 2000     # 每经过 {} 个时间步记录一次训练统计
t_max: 10000                   # 达到该时间步数后停止训练
use_cuda: True                 # 默认使用 GPU（如果可用）
buffer_cpu_only: True          # 如果为 True，则回放缓冲区仅保存在 CPU 内存中，不占用显存

# --- 日志选项 ---
use_tensorboard: True          # 将结果记录到 TensorBoard
save_model: False              # 是否保存模型到磁盘
save_model_interval: 2000000   # 每经过该时间步数后保存一次模型
checkpoint_path: ""            # 从该路径加载检查点
evaluate: False                # 评估模型：运行 test_nepisode 个回合后退出（不进行训练）
load_step: 0                   # 加载训练到指定时间步的模型（0 表示加载最大可用步数）
save_replay: False             # 是否保存从检查点加载的模型回放
local_results_path: "results"   # 本地结果保存路径

# --- 强化学习超参数 ---
gamma: 0.99
batch_size: 32                 # 用于训练的回合数
buffer_size: 32                # 回放缓冲区大小
lr: 0.0005                     # 智能体的学习率
critic_lr: 0.0005              # 评估器（critic）的学习率
optim_alpha: 0.99              # RMSProp 优化器的 alpha 参数
optim_eps: 0.00001             # RMSProp 优化器的 epsilon 参数
grad_norm_clip: 10             # 梯度 L2 范数超过该值时进行截断

# --- 智能体参数 ---
agent: "rnn"                   # 默认使用 RNN 智能体
rnn_hidden_dim: 64             # 默认 RNN 智能体的隐藏层维度
obs_agent_id: True             # 观测中是否包含智能体的 one-hot 编码 ID
obs_last_action: True          # 观测中是否包含智能体上一次的动作（one-hot 编码）

# --- 实验运行参数 ---
repeat_id: 1
label: "default_label"         # 实验标签
